<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cdekort.github.io/github-search-rss/dinov3-repos.rss</id>
    <title>DINOv3 Repositories</title>
    <updated>2026-02-08T00:41:34.587Z</updated>
    <generator>github-search-rss</generator>
    <link rel="alternate" href="https://cdekort.github.io/github-search-rss/dinov3-repos.rss"/>
    <subtitle>DINOv3 Repositories on GitHub</subtitle>
    <rights>github-search-rss</rights>
    <entry>
        <title type="html"><![CDATA[Nacho4412gg/dinov3]]></title>
        <id>https://github.com/Nacho4412gg/dinov3</id>
        <link href="https://github.com/Nacho4412gg/dinov3"/>
        <updated>2026-02-07T22:54:29.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/204909044?v=4" width="64" height="64" alt=""/><br/><div></div>]]></content>
        <author>
            <name>Nacho4412gg</name>
            <email>Nacho4412gg@noreply.github.com</email>
            <uri>https://github.com/Nacho4412gg</uri>
        </author>
        <published>2026-01-11T18:24:22.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[dlidgett/AD-DINOv3: üîç Enhance anomaly detection with AD-DINOv3, a framework adapting DINOv3 for zero-shot scenarios through advanced calibration techniques.]]></title>
        <id>https://github.com/dlidgett/AD-DINOv3</id>
        <link href="https://github.com/dlidgett/AD-DINOv3"/>
        <updated>2026-02-07T21:51:03.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/66205403?v=4" width="64" height="64" alt=""/><br/><div>üîç Enhance anomaly detection with AD-DINOv3, a framework adapting DINOv3 for zero-shot scenarios through advanced calibration techniques.</div>]]></content>
        <author>
            <name>dlidgett</name>
            <email>dlidgett@noreply.github.com</email>
            <uri>https://github.com/dlidgett</uri>
        </author>
        <published>2020-05-31T11:42:52.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[PhilaMaster/semantic_correspondence: This project evaluates and fine-tunes models like DINOv2, DINOv3, and Segment Anything Model (SAM) for dense matching tasks, enabling precise keypoint matching across semantically similar images.]]></title>
        <id>https://github.com/PhilaMaster/semantic_correspondence</id>
        <link href="https://github.com/PhilaMaster/semantic_correspondence"/>
        <updated>2026-02-07T16:23:04.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/77679627?v=4" width="64" height="64" alt=""/><br/><div>This project evaluates and fine-tunes models like DINOv2, DINOv3, and Segment Anything Model (SAM) for dense matching tasks, enabling precise keypoint matching across semantically similar images.</div>]]></content>
        <author>
            <name>PhilaMaster</name>
            <email>PhilaMaster@noreply.github.com</email>
            <uri>https://github.com/PhilaMaster</uri>
        </author>
        <published>2025-12-10T18:01:11.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[talhabw/autoseg: an annotation tool for segmentation models, with SAM3 integration and automatic propagation using image encoder models (DINOv3 and Pixio)]]></title>
        <id>https://github.com/talhabw/autoseg</id>
        <link href="https://github.com/talhabw/autoseg"/>
        <updated>2026-02-07T14:22:54.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/56639619?u=1a98caa2017443e235e7b84fc18a7182ec8a8a18&v=4" width="64" height="64" alt=""/><br/><div>an annotation tool for segmentation models, with SAM3 integration and automatic propagation using image encoder models (DINOv3 and Pixio)</div>]]></content>
        <author>
            <name>talhabw</name>
            <email>talhabw@noreply.github.com</email>
            <uri>https://github.com/talhabw</uri>
        </author>
        <published>2025-12-27T19:05:03.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[lightly-ai/lightly-train: All-in-one training for vision models (YOLO, ViTs, RT-DETR, DINOv3): pretraining, fine-tuning, distillation.]]></title>
        <id>https://github.com/lightly-ai/lightly-train</id>
        <link href="https://github.com/lightly-ai/lightly-train"/>
        <updated>2026-02-07T20:01:05.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/50146475?v=4" width="64" height="64" alt=""/><br/><div>All-in-one training for vision models (YOLO, ViTs, RT-DETR, DINOv3): pretraining, fine-tuning, distillation.</div>]]></content>
        <author>
            <name>lightly-ai</name>
            <email>lightly-ai@noreply.github.com</email>
            <uri>https://github.com/lightly-ai</uri>
        </author>
        <published>2025-04-10T08:23:51.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[GIS-Remote-Sensing-Goettingen/Dinov3-LWF-Segmentation]]></title>
        <id>https://github.com/GIS-Remote-Sensing-Goettingen/Dinov3-LWF-Segmentation</id>
        <link href="https://github.com/GIS-Remote-Sensing-Goettingen/Dinov3-LWF-Segmentation"/>
        <updated>2026-02-06T16:58:12.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/231213492?v=4" width="64" height="64" alt=""/><br/><div></div>]]></content>
        <author>
            <name>GIS-Remote-Sensing-Goettingen</name>
            <email>GIS-Remote-Sensing-Goettingen@noreply.github.com</email>
            <uri>https://github.com/GIS-Remote-Sensing-Goettingen</uri>
        </author>
        <published>2026-02-02T13:58:10.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[quanpham3105/DINOv3-Tracking-Research-CPX-Lab-]]></title>
        <id>https://github.com/quanpham3105/DINOv3-Tracking-Research-CPX-Lab-</id>
        <link href="https://github.com/quanpham3105/DINOv3-Tracking-Research-CPX-Lab-"/>
        <updated>2026-02-06T00:31:53.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/61221633?v=4" width="64" height="64" alt=""/><br/><div></div>]]></content>
        <author>
            <name>quanpham3105</name>
            <email>quanpham3105@noreply.github.com</email>
            <uri>https://github.com/quanpham3105</uri>
        </author>
        <published>2025-10-29T21:11:13.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[LukaDarsalia/colormnet_dinov3]]></title>
        <id>https://github.com/LukaDarsalia/colormnet_dinov3</id>
        <link href="https://github.com/LukaDarsalia/colormnet_dinov3"/>
        <updated>2026-02-05T23:47:05.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/29729836?u=d5b98a781f2e3e93ae1b3b9229177256600b097a&v=4" width="64" height="64" alt=""/><br/><div></div>]]></content>
        <author>
            <name>LukaDarsalia</name>
            <email>LukaDarsalia@noreply.github.com</email>
            <uri>https://github.com/LukaDarsalia</uri>
        </author>
        <published>2026-02-03T17:34:48.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[Raessan/dinov3_deepstream: DeepStream integration of Meta‚Äôs DINOv3 backbone with lightweight heads for vision tasks. ]]></title>
        <id>https://github.com/Raessan/dinov3_deepstream</id>
        <link href="https://github.com/Raessan/dinov3_deepstream"/>
        <updated>2026-02-07T20:30:38.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/81997218?u=5b16f2f94a92448943c16f5a3eda9597dff2f44a&v=4" width="64" height="64" alt=""/><br/><div>DeepStream integration of Meta‚Äôs DINOv3 backbone with lightweight heads for vision tasks. </div>]]></content>
        <author>
            <name>Raessan</name>
            <email>Raessan@noreply.github.com</email>
            <uri>https://github.com/Raessan</uri>
        </author>
        <published>2026-01-02T20:12:47.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[GIS-Remote-Sensing-Goettingen/dinov3-LWF-Segmentation-erosion]]></title>
        <id>https://github.com/GIS-Remote-Sensing-Goettingen/dinov3-LWF-Segmentation-erosion</id>
        <link href="https://github.com/GIS-Remote-Sensing-Goettingen/dinov3-LWF-Segmentation-erosion"/>
        <updated>2026-02-05T18:05:23.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/231213492?v=4" width="64" height="64" alt=""/><br/><div></div>]]></content>
        <author>
            <name>GIS-Remote-Sensing-Goettingen</name>
            <email>GIS-Remote-Sensing-Goettingen@noreply.github.com</email>
            <uri>https://github.com/GIS-Remote-Sensing-Goettingen</uri>
        </author>
        <published>2026-01-26T13:43:19.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[johnamit/motoreid: A deep learning pipeline for MotoGP team detection, tracking, and re-identification from race broadcast footage. This system combines YOLOv8 for robust object detection with DINOv3 (Vision Transformer) embeddings for semantic team classification.]]></title>
        <id>https://github.com/johnamit/motoreid</id>
        <link href="https://github.com/johnamit/motoreid"/>
        <updated>2026-02-05T15:09:02.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/75620312?u=8806c49054eb4620ae51aa548c5f08997486b21e&v=4" width="64" height="64" alt=""/><br/><div>A deep learning pipeline for MotoGP team detection, tracking, and re-identification from race broadcast footage. This system combines YOLOv8 for robust object detection with DINOv3 (Vision Transformer) embeddings for semantic team classification.</div>]]></content>
        <author>
            <name>johnamit</name>
            <email>johnamit@noreply.github.com</email>
            <uri>https://github.com/johnamit</uri>
        </author>
        <published>2026-01-14T18:42:24.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[RomanRes/CSIRO-Biomass-Prediction-using-DINOv3: The project focuses on modern computer vision techniques and demonstrates how large self-supervised vision transformers can be adapted efficiently for regression tasks using parameter-efficient fine-tuning.]]></title>
        <id>https://github.com/RomanRes/CSIRO-Biomass-Prediction-using-DINOv3</id>
        <link href="https://github.com/RomanRes/CSIRO-Biomass-Prediction-using-DINOv3"/>
        <updated>2026-02-05T13:17:20.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/62020123?u=46315ce64061756a2b722c9dc3857a3ff440baae&v=4" width="64" height="64" alt=""/><br/><div>The project focuses on modern computer vision techniques and demonstrates how large self-supervised vision transformers can be adapted efficiently for regression tasks using parameter-efficient fine-tuning.</div>]]></content>
        <author>
            <name>RomanRes</name>
            <email>RomanRes@noreply.github.com</email>
            <uri>https://github.com/RomanRes</uri>
        </author>
        <published>2026-02-02T19:49:23.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[jayd972/Multi-Agent-dinov3-explainable-tumor-detection-framework]]></title>
        <id>https://github.com/jayd972/Multi-Agent-dinov3-explainable-tumor-detection-framework</id>
        <link href="https://github.com/jayd972/Multi-Agent-dinov3-explainable-tumor-detection-framework"/>
        <updated>2026-02-05T11:33:04.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/75724261?u=ec9417a3cccd67a77f38c9ae073a07dd8af99e9e&v=4" width="64" height="64" alt=""/><br/><div></div>]]></content>
        <author>
            <name>jayd972</name>
            <email>jayd972@noreply.github.com</email>
            <uri>https://github.com/jayd972</uri>
        </author>
        <published>2026-02-05T11:01:15.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[wen146146/dinov3-detr]]></title>
        <id>https://github.com/wen146146/dinov3-detr</id>
        <link href="https://github.com/wen146146/dinov3-detr"/>
        <updated>2026-02-05T07:15:46.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/250138186?v=4" width="64" height="64" alt=""/><br/><div></div>]]></content>
        <author>
            <name>wen146146</name>
            <email>wen146146@noreply.github.com</email>
            <uri>https://github.com/wen146146</uri>
        </author>
        <published>2026-02-05T00:52:44.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thanghuynh2808/MORAD_MAI: Automated Planogram Compliance system for retail shelves. Features: Panorama Stitching, Zero-shot SKU Detection (YOLO, DINOv3, LightGlue), and LLM-powered rule validation. Optimized for large-scale grocery shelf analysis in Vietnam.]]></title>
        <id>https://github.com/Thanghuynh2808/MORAD_MAI</id>
        <link href="https://github.com/Thanghuynh2808/MORAD_MAI"/>
        <updated>2026-02-04T04:12:25.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/156525690?u=859091d4e8fe94e40ba8c4ba1324563f3228fda3&v=4" width="64" height="64" alt=""/><br/><div>Automated Planogram Compliance system for retail shelves. Features: Panorama Stitching, Zero-shot SKU Detection (YOLO, DINOv3, LightGlue), and LLM-powered rule validation. Optimized for large-scale grocery shelf analysis in Vietnam.</div>]]></content>
        <author>
            <name>Thanghuynh2808</name>
            <email>Thanghuynh2808@noreply.github.com</email>
            <uri>https://github.com/Thanghuynh2808</uri>
        </author>
        <published>2026-02-03T02:40:51.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[yberreby/dinov3-in1k-probes: Pretrained ImageNet-1k linear classification heads for DINOv3 ViT (S/S+/B/L/H+).]]></title>
        <id>https://github.com/yberreby/dinov3-in1k-probes</id>
        <link href="https://github.com/yberreby/dinov3-in1k-probes"/>
        <updated>2026-02-04T03:36:59.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/5556521?u=d6ca49ea6294af95c95c75ad2c94620fd8950381&v=4" width="64" height="64" alt=""/><br/><div>Pretrained ImageNet-1k linear classification heads for DINOv3 ViT (S/S+/B/L/H+).</div>]]></content>
        <author>
            <name>yberreby</name>
            <email>yberreby@noreply.github.com</email>
            <uri>https://github.com/yberreby</uri>
        </author>
        <published>2025-11-16T20:03:12.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[aihpi/tool-dinov3-embeddings-api: FastAPI service for DINOv3 image embeddings, containerized for GPU/CPU and Kubernetes.]]></title>
        <id>https://github.com/aihpi/tool-dinov3-embeddings-api</id>
        <link href="https://github.com/aihpi/tool-dinov3-embeddings-api"/>
        <updated>2026-02-03T19:00:58.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/149687007?v=4" width="64" height="64" alt=""/><br/><div>FastAPI service for DINOv3 image embeddings, containerized for GPU/CPU and Kubernetes.</div>]]></content>
        <author>
            <name>aihpi</name>
            <email>aihpi@noreply.github.com</email>
            <uri>https://github.com/aihpi</uri>
        </author>
        <published>2026-02-03T15:25:45.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[tb-tian/AIC2025-VideoRetrieval-float97: Multimodal video retrieval system for HCMC AI Challenge 2025 | SigLIP Keyframe extraction + DINOv3 + WhisperX]]></title>
        <id>https://github.com/tb-tian/AIC2025-VideoRetrieval-float97</id>
        <link href="https://github.com/tb-tian/AIC2025-VideoRetrieval-float97"/>
        <updated>2026-02-03T16:53:45.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/145255003?v=4" width="64" height="64" alt=""/><br/><div>Multimodal video retrieval system for HCMC AI Challenge 2025 | SigLIP Keyframe extraction + DINOv3 + WhisperX</div>]]></content>
        <author>
            <name>tb-tian</name>
            <email>tb-tian@noreply.github.com</email>
            <uri>https://github.com/tb-tian</uri>
        </author>
        <published>2025-07-23T11:51:00.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[zgcr/SimpleAICV_pytorch_training_examples: SimpleAICV:pytorch training examples.]]></title>
        <id>https://github.com/zgcr/SimpleAICV_pytorch_training_examples</id>
        <link href="https://github.com/zgcr/SimpleAICV_pytorch_training_examples"/>
        <updated>2026-02-06T07:27:05.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/19390485?u=d9f77e6cad56ea1c0c19ea2852b7833c76d2cf4a&v=4" width="64" height="64" alt=""/><br/><div>SimpleAICV:pytorch training examples.</div>]]></content>
        <author>
            <name>zgcr</name>
            <email>zgcr@noreply.github.com</email>
            <uri>https://github.com/zgcr</uri>
        </author>
        <published>2020-05-31T05:37:26.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sonam525/PEFT-Fine-tuning-cows: Exploring the power of PEFT by systematically comparing three approaches for nine-class dairy cow behavior classification: training from scratch (ResNet-18, ViT-Small), frozen feature extraction, and parameter-efficient fine-tuning (PEFT) of the DINOv3 foundation model (6.7 billion parameters)]]></title>
        <id>https://github.com/Sonam525/PEFT-Fine-tuning-cows</id>
        <link href="https://github.com/Sonam525/PEFT-Fine-tuning-cows"/>
        <updated>2026-02-02T18:18:50.000Z</updated>
        <content type="html"><![CDATA[<img src="https://avatars.githubusercontent.com/u/149631650?u=f3c37eed93aa20ed0f7b8f11833db0043b1ae9a2&v=4" width="64" height="64" alt=""/><br/><div>Exploring the power of PEFT by systematically comparing three approaches for nine-class dairy cow behavior classification: training from scratch (ResNet-18, ViT-Small), frozen feature extraction, and parameter-efficient fine-tuning (PEFT) of the DINOv3 foundation model (6.7 billion parameters)</div>]]></content>
        <author>
            <name>Sonam525</name>
            <email>Sonam525@noreply.github.com</email>
            <uri>https://github.com/Sonam525</uri>
        </author>
        <published>2026-02-02T14:04:04.000Z</published>
    </entry>
</feed>